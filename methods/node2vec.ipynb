{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Summary\n",
    "\n",
    "- Network: $G = (V, E)$\n",
    "- feature representations: $f: V \\rightarrow \\mathcal{R}^d$\n",
    "    - $d$: number of dimensions of feature representations\n",
    "    - $\\mathbf{f} \\in \\mathcal{R}^{n\\times d} = [f(v_1), \\dots, f(v_n)]^\\top$\n",
    "        - $v_i \\in V$\n",
    "        - $n = |V|$\n",
    "\n",
    "- Network neighborhood: $N_S(v) \\subset V$\n",
    "    - $v \\in V$\n",
    "    - Sampling strategy: $S$\n",
    "\n",
    "- Objective function: maximum log conditional likelihood with two assumptions\n",
    "   $$\\max_f \\sum_{v\\in V} \\log{P(N_S(v) | f(v))} = \\max_f \\sum_{v\\in V} \\left[ -\\log{\\sum_{u \\in V} e^{f(u)f(v)}} +  \\sum_{v' \\in N_S(v)} f(v') f(v) \\right]$$\n",
    "\n",
    "    - Conditional independence: $P(N_S(v) | f(v)) = \\prod_{u' \\in N_S(v)} P(v' | f(v))$\n",
    "    - Softmax parameterization (with on symmetric feature space): $P(v' | f(v)) = \\frac{e^{f(v')f(v)}}{\\sum_{u \\in V} e^{f(u)f(v)}}$\n",
    "- Computation concerns\n",
    "    1. $\\sum_{u \\in V} e^{f(u)f(v)}$ is expensive when the network is huge $\\rightarrow$ Approximate it by negative sampling.\n",
    "    2. Use stochastic gradient ascent to solve the right-handed side of the above objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
