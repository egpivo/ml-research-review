{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Summary\n",
    "\n",
    "### Basic Settings\n",
    "- Network: $G = (V, E)$\n",
    "- Feature representations: $f: V \\rightarrow \\mathcal{R}^d$\n",
    "    - $d$: number of dimensions of feature representations\n",
    "    - $\\mathbf{f} \\in \\mathcal{R}^{n\\times d} = [f(v_1), \\dots, f(v_n)]^\\top$\n",
    "        - $v_i \\in V$\n",
    "        - $n = |V|$\n",
    "\n",
    "- Network neighborhood: $N_S(v) \\subset V$\n",
    "    - $v \\in V$\n",
    "    - Sampling strategy: $S$\n",
    "\n",
    "### Feature Learning Problem\n",
    "- Objective function: maximum log conditional likelihood of $f$ with two assumptions\n",
    "   $$\\max_f \\sum_{v\\in V} \\log{P(N_S(v) | f(v))} = \\max_f \\sum_{v\\in V} \\left[ -\\log{\\sum_{u \\in V} e^{f(u)f(v)}} +  \\sum_{v' \\in N_S(v)} f(v') f(v) \\right]$$\n",
    "\n",
    "    - Conditional independence: $P(N_S(v) | f(v)) = \\prod_{u' \\in N_S(v)} P(v' | f(v))$\n",
    "    - Softmax parameterization (with on symmetric feature space): $P(v' | f(v)) = \\frac{e^{f(v')f(v)}}{\\sum_{u \\in V} e^{f(u)f(v)}}$\n",
    "- Computation concerns\n",
    "    1. $\\sum_{u \\in V} e^{f(u)f(v)}$ is expensive when the network is huge $\\rightarrow$ Approximate it by negative sampling.\n",
    "    2. Use stochastic gradient ascent to solve the right-handed side of the above objective function.\n",
    "\n",
    "### Sampling Strategy\n",
    "- The problem of sampling neighbors of a source node as a form of local search. \n",
    "- Aim to generate a node's ($v$) neighborhodd $N_S(v)$\n",
    "- Example\n",
    "   - Breadth-First Sampling (BFS)\n",
    "   - Depth-First Sampling (DFS)\n",
    "- Two common hypotheses\n",
    "   - homophily:\n",
    "     -  ***highly interconnected*** and belong to ***similar network*** clusters or communities should be embedded closely together.\n",
    "     - macro-view of the neighborhood of every node\n",
    "     - e.g., DFS\n",
    "   - strctural equivalance:\n",
    "      -  ***similar structure roles*** in networks should be embedded closely together.\n",
    "      - not emphasize connectivity\n",
    "      - microscopiv view of the neighborhood of every node\n",
    "      - e.g., BFS\n",
    "\n",
    "#### Random walks\n",
    "  $$P(c_i = v' | c_{i-1} = v) = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "    \\frac{\\pi_{vv'}}{Z},& (v, v') \\in E\\\\\n",
    "    0,& o.w.\n",
    "    \\end{array}\n",
    "  \\right.$$\n",
    "   - $c_i$: $i$th node in the walk\n",
    "   - $c_0 = u$ is the starting node\n",
    "   - $\\pi_{vv'}$: unnormalized transition probability between $v$ and $v'$\n",
    "      - $\\pi_{vv'}$ can be static edge weight $\\omega_{vv'}$\n",
    "   - $Z$: normalizing constant\n",
    "\n",
    "- Second order random walks with search bias $\\alpha_{pq} \\in [0, 1]$. $\\pi_{vv'} = \\alpha_{pq}(t, v')\\omega_{vv'}$\n",
    "    $$\\alpha_{pq}(t, v') = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            \\frac{1}{p},&& d_{tv'} = 0\\\\\n",
    "            1,&& d_{tv'} = 1\\\\\n",
    "            \\frac{1}{q},&& d_{tv'} = 2\n",
    "        \\end{array}\n",
    "    \\right.$$\n",
    "    - $p, q$: parameters control how fast the walk explores and leaves the neighborhood of starting node $u$\n",
    "        - return parameter $p$\n",
    "           - if $p$ is low, the walk backtracks a step, i.e., keep the walk local close to $u$\n",
    "        - in-out parameter $q$\n",
    "            - if $q > 1$, the random walk is biased towards nodes close to node $t$ (aproximate BFS)\n",
    "            - if $q < 1$, the random walk is inclined to visit nodes far away from node $t$ (aproximate DFS)\n",
    "\n",
    "    - $d_{tv'} \\in \\{0, 1, 2\\}$: shortest path distance between nodes $t$ and $v'$\n",
    "\n",
    "\n",
    "\n",
    "#### Algorithm\n",
    "- Three phases of `node2vec`\n",
    "   1. preprocessing to compute transition probabilities\n",
    "   2. generate random walks\n",
    "   3. optimize the objective with SGD\n",
    "\n",
    "- Algorithm\n",
    "    - Input\n",
    "        - Graph $G = (V, E, W)$\n",
    "        - Dimensions (embedding size): $d$\n",
    "        - Walk node size: $r$\n",
    "        - Walk length $l$\n",
    "        - Context size: $k$\n",
    "        - Return parameter $p$\n",
    "        - In-out parameter $q$\n",
    "    - Output: $\\mathbf{f} \\in \\mathcal{R}^{|V|\\times d}$\n",
    "    - Flow\n",
    "      1. Cacluate $\\pi = \\text{PreporcessModifiedWeights}(G, p, q)$\n",
    "      2. Create $G' = (V, E, \\pi)$\n",
    "      3. Create `walks = [node2vecWalk(G', v, l) for iter, v in zip(range(r), V)]`\n",
    "      4. Compute `f = SGD(k, d, walks)`\n",
    "    - Helper functions\n",
    "        - Generate a random walk : `node2vecWalk(G = (V, E, \\pi): graph, start_node, walk_length)`\n",
    "            1. Initialize `walk = [start_node]`\n",
    "            2. `for iter in range(walk_length):`\n",
    "               1. `current_node = walk[-1]`\n",
    "               2. `V_current = GetNeighbors(current_node, G)`\n",
    "               3. `next_node = AliasSample(V_current, \\pi)`\n",
    "               4. `walk.append(next_node)`\n",
    "            3. Return `walk`\n",
    "         - Fetch neighbors by a node `GetNeighbors(node, graph)`\n",
    "         - Sample a node by transition probability `AliasSample(node, \\pi)`\n",
    "       \n",
    "       \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
